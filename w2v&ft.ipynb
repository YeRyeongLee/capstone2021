{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2v&ft.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnTdelJCPcdL",
        "outputId": "6e2fcc0b-bfee-4624-a1b2-47e4f85b0edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.1.2 in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (5.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim==4.1.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J91FHIEI8HWk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3siJi2by8Zu7",
        "outputId": "23a53a43-1a08-4709-f11b-0a866add327c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_morphs = np.load(\"/gdrive/My Drive/Colab Notebooks/final_X_morphs.npy\", allow_pickle=True)\n",
        "y_morphs = np.load(\"/gdrive/My Drive/Colab Notebooks/final_y_morphs.npy\", allow_pickle=True)"
      ],
      "metadata": {
        "id": "ZDR-WakHWe89"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "enc = LabelEncoder()\n",
        "y_morphs = enc.fit_transform(y_morphs)"
      ],
      "metadata": {
        "id": "z6KHDBs6bj0Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xm_train, Xm_test, ym_train, ym_test = train_test_split(X_morphs, y_morphs, test_size=0.3, random_state=777, stratify=y_morphs)"
      ],
      "metadata": {
        "id": "8vuP8p8qbnzv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "ft = FastText(sentences=Xm_train, vector_size=100, window=2, min_count=5, sg=1)\n",
        "w2v = Word2Vec(sentences=Xm_train, vector_size=100, window=2, min_count=5, sg=1)"
      ],
      "metadata": {
        "id": "2p81WXJ6c8ST"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.wv.most_similar('눈물')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2giUBaSCdaAo",
        "outputId": "f4446e16-cfe7-494b-d21a-1ac41ed6a15e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('자꾸', 0.9706909656524658),\n",
              " ('그렇게', 0.9573221802711487),\n",
              " ('조금', 0.9549165964126587),\n",
              " ('맘', 0.9537326097488403),\n",
              " ('갑자기', 0.9529294967651367),\n",
              " ('이렇게', 0.9521529078483582),\n",
              " ('겁', 0.9513979554176331),\n",
              " ('본적', 0.9491633772850037),\n",
              " ('기억', 0.9474244713783264),\n",
              " ('처음', 0.9472329020500183)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft.wv.most_similar('눈물')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_FyhVONc_Xw",
        "outputId": "8a5e877f-2956-49f3-a5a4-fdf746fcc561"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('가슴', 0.9452393054962158),\n",
              " ('빠르다', 0.9356359839439392),\n",
              " ('괜히', 0.9344004392623901),\n",
              " ('자꾸', 0.9306739568710327),\n",
              " ('오르다', 0.928200364112854),\n",
              " ('생각나다', 0.9254056811332703),\n",
              " ('눈물나다', 0.9247233271598816),\n",
              " ('두렵다', 0.9235146045684814),\n",
              " ('조금', 0.9216457605361938),\n",
              " ('갑자기', 0.9202181696891785)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.wv.most_similar('더럽다')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWN4dOLMc_af",
        "outputId": "723723a7-4d69-474b-f48a-5c3318d31fed"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('억울하다', 0.9859548211097717),\n",
              " ('선동', 0.9854364991188049),\n",
              " ('벌레', 0.984391450881958),\n",
              " ('권력', 0.9842827320098877),\n",
              " ('미개하다', 0.9842439889907837),\n",
              " ('공무원', 0.9841915965080261),\n",
              " ('국회의원', 0.9834848642349243),\n",
              " ('하나같이', 0.982538104057312),\n",
              " ('사기꾼', 0.9817013144493103),\n",
              " ('똑같이', 0.9816969037055969)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft.wv.most_similar('더럽다')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzmSlaSSc_dB",
        "outputId": "ffa4eeb7-cd8c-4d70-dd7b-b21a9771fe73"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('미개하다', 0.9711623787879944),\n",
              " ('도둑', 0.9690681099891663),\n",
              " ('설치다', 0.9689338207244873),\n",
              " ('끼리', 0.9660525321960449),\n",
              " ('넘치다', 0.9656282067298889),\n",
              " ('똑똑하다', 0.9646580219268799),\n",
              " ('뭉치다', 0.9635692834854126),\n",
              " ('진정', 0.9632050395011902),\n",
              " ('닥치다', 0.9620828032493591),\n",
              " ('망치다', 0.9615488052368164)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(Xm_train)\n",
        "sequences = tokenizer.texts_to_sequences(Xm_train)\n",
        "Xm_test_tok = tokenizer.texts_to_sequences(Xm_test)\n",
        "\n",
        "Xm_train_pad = pad_sequences(sequences, maxlen=30)\n",
        "Xm_test_pad = pad_sequences(Xm_test_tok, maxlen=30)"
      ],
      "metadata": {
        "id": "A7XiecXYbYf3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(tokenizer.index_word) + 1\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
        "\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "    embedding_vector = ft.wv[word] if word in ft.wv else None\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "        \n",
        "embedding_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k09fYRtaZfoh",
        "outputId": "543b7dab-8d7b-4e99-f6b2-2fdc5b59782d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20043, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, InputLayer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "hidden_units = 128\n",
        "num_classes = 7\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4, restore_best_weights=True)\n",
        "mc = ModelCheckpoint('/gdrive/My Drive/Colab Notebooks/ft_lstm.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(Xm_train_pad, ym_train, epochs=20, callbacks=[es, mc], batch_size=128, validation_split=0.15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVUVrTjQjRBZ",
        "outputId": "c6e55459-fe39-462d-c013-b73d90dd60ea"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.6357 - acc: 0.3544\n",
            "Epoch 00001: val_acc improved from -inf to 0.39157, saving model to /gdrive/My Drive/Colab Notebooks/ft_lstm.h5\n",
            "179/179 [==============================] - 27s 124ms/step - loss: 1.6357 - acc: 0.3544 - val_loss: 1.5471 - val_acc: 0.3916\n",
            "Epoch 2/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.5164 - acc: 0.4066\n",
            "Epoch 00002: val_acc improved from 0.39157 to 0.40818, saving model to /gdrive/My Drive/Colab Notebooks/ft_lstm.h5\n",
            "179/179 [==============================] - 19s 105ms/step - loss: 1.5164 - acc: 0.4066 - val_loss: 1.5234 - val_acc: 0.4082\n",
            "Epoch 3/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4943 - acc: 0.4149\n",
            "Epoch 00003: val_acc did not improve from 0.40818\n",
            "179/179 [==============================] - 19s 105ms/step - loss: 1.4943 - acc: 0.4149 - val_loss: 1.5141 - val_acc: 0.4055\n",
            "Epoch 4/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4784 - acc: 0.4228\n",
            "Epoch 00004: val_acc did not improve from 0.40818\n",
            "179/179 [==============================] - 19s 104ms/step - loss: 1.4784 - acc: 0.4228 - val_loss: 1.5071 - val_acc: 0.4079\n",
            "Epoch 5/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4692 - acc: 0.4276\n",
            "Epoch 00005: val_acc improved from 0.40818 to 0.41859, saving model to /gdrive/My Drive/Colab Notebooks/ft_lstm.h5\n",
            "179/179 [==============================] - 19s 106ms/step - loss: 1.4692 - acc: 0.4276 - val_loss: 1.4917 - val_acc: 0.4186\n",
            "Epoch 6/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4548 - acc: 0.4301\n",
            "Epoch 00006: val_acc did not improve from 0.41859\n",
            "179/179 [==============================] - 19s 108ms/step - loss: 1.4548 - acc: 0.4301 - val_loss: 1.4954 - val_acc: 0.4129\n",
            "Epoch 7/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4431 - acc: 0.4350\n",
            "Epoch 00007: val_acc did not improve from 0.41859\n",
            "179/179 [==============================] - 19s 104ms/step - loss: 1.4431 - acc: 0.4350 - val_loss: 1.4822 - val_acc: 0.4169\n",
            "Epoch 8/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4334 - acc: 0.4404\n",
            "Epoch 00008: val_acc improved from 0.41859 to 0.41884, saving model to /gdrive/My Drive/Colab Notebooks/ft_lstm.h5\n",
            "179/179 [==============================] - 19s 107ms/step - loss: 1.4334 - acc: 0.4404 - val_loss: 1.4815 - val_acc: 0.4188\n",
            "Epoch 9/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4240 - acc: 0.4460\n",
            "Epoch 00009: val_acc improved from 0.41884 to 0.42082, saving model to /gdrive/My Drive/Colab Notebooks/ft_lstm.h5\n",
            "179/179 [==============================] - 19s 106ms/step - loss: 1.4240 - acc: 0.4460 - val_loss: 1.4817 - val_acc: 0.4208\n",
            "Epoch 10/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4164 - acc: 0.4475\n",
            "Epoch 00010: val_acc improved from 0.42082 to 0.42751, saving model to /gdrive/My Drive/Colab Notebooks/ft_lstm.h5\n",
            "179/179 [==============================] - 19s 107ms/step - loss: 1.4164 - acc: 0.4475 - val_loss: 1.4768 - val_acc: 0.4275\n",
            "Epoch 11/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4090 - acc: 0.4514\n",
            "Epoch 00011: val_acc did not improve from 0.42751\n",
            "179/179 [==============================] - 19s 107ms/step - loss: 1.4090 - acc: 0.4514 - val_loss: 1.4699 - val_acc: 0.4273\n",
            "Epoch 12/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3965 - acc: 0.4574\n",
            "Epoch 00012: val_acc did not improve from 0.42751\n",
            "179/179 [==============================] - 19s 107ms/step - loss: 1.3965 - acc: 0.4574 - val_loss: 1.4642 - val_acc: 0.4228\n",
            "Epoch 13/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3849 - acc: 0.4632\n",
            "Epoch 00013: val_acc improved from 0.42751 to 0.43172, saving model to /gdrive/My Drive/Colab Notebooks/ft_lstm.h5\n",
            "179/179 [==============================] - 23s 130ms/step - loss: 1.3849 - acc: 0.4632 - val_loss: 1.4708 - val_acc: 0.4317\n",
            "Epoch 14/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3744 - acc: 0.4663\n",
            "Epoch 00014: val_acc did not improve from 0.43172\n",
            "179/179 [==============================] - 29s 163ms/step - loss: 1.3744 - acc: 0.4663 - val_loss: 1.4648 - val_acc: 0.4270\n",
            "Epoch 15/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3631 - acc: 0.4701\n",
            "Epoch 00015: val_acc did not improve from 0.43172\n",
            "179/179 [==============================] - 31s 173ms/step - loss: 1.3631 - acc: 0.4701 - val_loss: 1.4576 - val_acc: 0.4265\n",
            "Epoch 16/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3483 - acc: 0.4792\n",
            "Epoch 00016: val_acc did not improve from 0.43172\n",
            "179/179 [==============================] - 29s 161ms/step - loss: 1.3483 - acc: 0.4792 - val_loss: 1.4736 - val_acc: 0.4233\n",
            "Epoch 17/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3374 - acc: 0.4807\n",
            "Epoch 00017: val_acc did not improve from 0.43172\n",
            "179/179 [==============================] - 19s 104ms/step - loss: 1.3374 - acc: 0.4807 - val_loss: 1.4714 - val_acc: 0.4228\n",
            "Epoch 18/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3207 - acc: 0.4881\n",
            "Epoch 00018: val_acc did not improve from 0.43172\n",
            "179/179 [==============================] - 19s 105ms/step - loss: 1.3207 - acc: 0.4881 - val_loss: 1.4802 - val_acc: 0.4302\n",
            "Epoch 19/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3051 - acc: 0.4944Restoring model weights from the end of the best epoch: 15.\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.43172\n",
            "179/179 [==============================] - 19s 105ms/step - loss: 1.3051 - acc: 0.4944 - val_loss: 1.4826 - val_acc: 0.4233\n",
            "Epoch 00019: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(tokenizer.index_word) + 1\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "embedding_matrix2 = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
        "\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "    embedding_vector = w2v.wv[word] if word in w2v.wv else None\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix2[idx] = embedding_vector\n",
        "        \n",
        "embedding_matrix2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DmAB74ckVp4",
        "outputId": "446f67ae-93dd-4017-b7bb-a4278df67967"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20043, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, InputLayer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "hidden_units = 128\n",
        "num_classes = 7\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, weights=[embedding_matrix2], trainable=False))\n",
        "model2.add(LSTM(hidden_units))\n",
        "model2.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4, restore_best_weights=True)\n",
        "mc = ModelCheckpoint('/gdrive/My Drive/Colab Notebooks/w2v_lstm.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "history = model2.fit(Xm_train_pad, ym_train, epochs=20, callbacks=[es, mc], batch_size=128, validation_split=0.15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgjVayMYpENy",
        "outputId": "2a7f1b41-c599-4112-c5f5-8801583a8d68"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.6489 - acc: 0.3503\n",
            "Epoch 00001: val_acc improved from -inf to 0.37348, saving model to /gdrive/My Drive/Colab Notebooks/w2v_lstm.h5\n",
            "179/179 [==============================] - 21s 106ms/step - loss: 1.6489 - acc: 0.3503 - val_loss: 1.6013 - val_acc: 0.3735\n",
            "Epoch 2/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.5628 - acc: 0.3833\n",
            "Epoch 00002: val_acc improved from 0.37348 to 0.37546, saving model to /gdrive/My Drive/Colab Notebooks/w2v_lstm.h5\n",
            "179/179 [==============================] - 19s 105ms/step - loss: 1.5628 - acc: 0.3833 - val_loss: 1.5824 - val_acc: 0.3755\n",
            "Epoch 3/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.5423 - acc: 0.3964\n",
            "Epoch 00003: val_acc improved from 0.37546 to 0.38786, saving model to /gdrive/My Drive/Colab Notebooks/w2v_lstm.h5\n",
            "179/179 [==============================] - 19s 105ms/step - loss: 1.5423 - acc: 0.3964 - val_loss: 1.5570 - val_acc: 0.3879\n",
            "Epoch 4/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.5212 - acc: 0.4043\n",
            "Epoch 00004: val_acc improved from 0.38786 to 0.40620, saving model to /gdrive/My Drive/Colab Notebooks/w2v_lstm.h5\n",
            "179/179 [==============================] - 19s 106ms/step - loss: 1.5212 - acc: 0.4043 - val_loss: 1.5466 - val_acc: 0.4062\n",
            "Epoch 5/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.5086 - acc: 0.4097\n",
            "Epoch 00005: val_acc did not improve from 0.40620\n",
            "179/179 [==============================] - 29s 161ms/step - loss: 1.5086 - acc: 0.4097 - val_loss: 1.5371 - val_acc: 0.4037\n",
            "Epoch 6/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4943 - acc: 0.4170\n",
            "Epoch 00006: val_acc did not improve from 0.40620\n",
            "179/179 [==============================] - 28s 159ms/step - loss: 1.4943 - acc: 0.4170 - val_loss: 1.5279 - val_acc: 0.4017\n",
            "Epoch 7/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4833 - acc: 0.4185\n",
            "Epoch 00007: val_acc did not improve from 0.40620\n",
            "179/179 [==============================] - 28s 156ms/step - loss: 1.4833 - acc: 0.4185 - val_loss: 1.5240 - val_acc: 0.4050\n",
            "Epoch 8/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4708 - acc: 0.4247\n",
            "Epoch 00008: val_acc improved from 0.40620 to 0.41165, saving model to /gdrive/My Drive/Colab Notebooks/w2v_lstm.h5\n",
            "179/179 [==============================] - 28s 157ms/step - loss: 1.4708 - acc: 0.4247 - val_loss: 1.5222 - val_acc: 0.4116\n",
            "Epoch 9/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4611 - acc: 0.4278\n",
            "Epoch 00009: val_acc did not improve from 0.41165\n",
            "179/179 [==============================] - 30s 168ms/step - loss: 1.4611 - acc: 0.4278 - val_loss: 1.5110 - val_acc: 0.4072\n",
            "Epoch 10/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4535 - acc: 0.4307\n",
            "Epoch 00010: val_acc did not improve from 0.41165\n",
            "179/179 [==============================] - 28s 156ms/step - loss: 1.4535 - acc: 0.4307 - val_loss: 1.5338 - val_acc: 0.4089\n",
            "Epoch 11/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4420 - acc: 0.4392\n",
            "Epoch 00011: val_acc did not improve from 0.41165\n",
            "179/179 [==============================] - 28s 158ms/step - loss: 1.4420 - acc: 0.4392 - val_loss: 1.5238 - val_acc: 0.4084\n",
            "Epoch 12/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4312 - acc: 0.4397\n",
            "Epoch 00012: val_acc improved from 0.41165 to 0.41239, saving model to /gdrive/My Drive/Colab Notebooks/w2v_lstm.h5\n",
            "179/179 [==============================] - 29s 161ms/step - loss: 1.4312 - acc: 0.4397 - val_loss: 1.5085 - val_acc: 0.4124\n",
            "Epoch 13/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4192 - acc: 0.4501\n",
            "Epoch 00013: val_acc did not improve from 0.41239\n",
            "179/179 [==============================] - 26s 147ms/step - loss: 1.4192 - acc: 0.4501 - val_loss: 1.5054 - val_acc: 0.4109\n",
            "Epoch 14/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.4062 - acc: 0.4554\n",
            "Epoch 00014: val_acc did not improve from 0.41239\n",
            "179/179 [==============================] - 21s 115ms/step - loss: 1.4062 - acc: 0.4554 - val_loss: 1.5142 - val_acc: 0.3948\n",
            "Epoch 15/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3998 - acc: 0.4571\n",
            "Epoch 00015: val_acc improved from 0.41239 to 0.41884, saving model to /gdrive/My Drive/Colab Notebooks/w2v_lstm.h5\n",
            "179/179 [==============================] - 25s 140ms/step - loss: 1.3998 - acc: 0.4571 - val_loss: 1.5038 - val_acc: 0.4188\n",
            "Epoch 16/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3847 - acc: 0.4636\n",
            "Epoch 00016: val_acc improved from 0.41884 to 0.42007, saving model to /gdrive/My Drive/Colab Notebooks/w2v_lstm.h5\n",
            "179/179 [==============================] - 25s 140ms/step - loss: 1.3847 - acc: 0.4636 - val_loss: 1.5040 - val_acc: 0.4201\n",
            "Epoch 17/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3697 - acc: 0.4698\n",
            "Epoch 00017: val_acc did not improve from 0.42007\n",
            "179/179 [==============================] - 21s 120ms/step - loss: 1.3697 - acc: 0.4698 - val_loss: 1.5060 - val_acc: 0.4144\n",
            "Epoch 18/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3529 - acc: 0.4730\n",
            "Epoch 00018: val_acc did not improve from 0.42007\n",
            "179/179 [==============================] - 27s 150ms/step - loss: 1.3529 - acc: 0.4730 - val_loss: 1.5132 - val_acc: 0.4126\n",
            "Epoch 19/20\n",
            "179/179 [==============================] - ETA: 0s - loss: 1.3379 - acc: 0.4837Restoring model weights from the end of the best epoch: 15.\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.42007\n",
            "179/179 [==============================] - 29s 160ms/step - loss: 1.3379 - acc: 0.4837 - val_loss: 1.5327 - val_acc: 0.4141\n",
            "Epoch 00019: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(Xm_test_pad, ym_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc5p3rzppIao",
        "outputId": "a1d4a761-7ddc-4699-e35e-d8ce48a559cf"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "361/361 [==============================] - 5s 13ms/step - loss: 1.4564 - acc: 0.4427\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4564201831817627, 0.4426613450050354]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.evaluate(Xm_test_pad, ym_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yycQPlmuffx",
        "outputId": "d20dc291-c83b-454a-82d6-43c76dc5b448"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "361/361 [==============================] - 5s 13ms/step - loss: 1.5001 - acc: 0.4192\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5001394748687744, 0.4192401170730591]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 표현\n",
        "sent_list = ['정말 행복해',\n",
        "             '정말 슬퍼',\n",
        "             '정말 무서워',\n",
        "             '정말 싫어',\n",
        "             '정말 화가 나',\n",
        "             '깜짝 놀랐어',\n",
        "             '종이는 하얗다']\n",
        "\n",
        "# 복잡한 표현\n",
        "sent_list2 = ['ㅋㅋㅋ 유재석 김태호 조합은 믿고 보는거지~',\n",
        "              '그저께 어머니가 돌아가시고 세상을 잃은 기분입니다..',\n",
        "              '아이가 어제부터 토를 계속 하는데 어떻게 해야하죠??',\n",
        "              '이놈이나 저놈이나 다 똑같은 놈들이야',\n",
        "              '범죄자들 얼굴을 왜 가리나? 신상공개하라!!',\n",
        "              '이런거보면 참 우주는 대단한 듯.. 37억이라는 세월.. 짐작도 안간다.',\n",
        "              '저는 법학도이고 현재는 로스쿨에 재학중입니다.']\n",
        "\n",
        "# 비문과 이모티콘\n",
        "sent_list3 = ['넘넘 추카해요~~~^^',\n",
        "              '하.... 나 시험 개망햇다...ㅠ..',\n",
        "              '나 이번달에 생리를 안하는데?? 어떡하징 ㄷㄷ',\n",
        "              '표절가수 얼굴 보기도 실타ㅋㅋ 나오지 마라',\n",
        "              '아 징짜 ㅡㅡ 초딩은 사람도 아님?',\n",
        "              '헐 마라탕 위생 문졔 있다구?? 나 마라탕 어제도 먹엇는데??',\n",
        "              '이번네 삼성에서 새로운 핸드폰이 출시됩비다.']"
      ],
      "metadata": {
        "id": "bDLO5qRbujPs"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = sent_list + sent_list2 + sent_list3"
      ],
      "metadata": {
        "id": "2x5Bf-Eou3jd"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEwXEoAEv_GU",
        "outputId": "e2315e16-e552-4879-c2c3-7c851f7e0e24"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 6.8 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 37.9 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "def sent_to_morphs(sent, stopwords, norm):\n",
        "  tokenized = okt.morphs(sent, stem=True)\n",
        "  sw_removed = [word for word in tokenized if not word in stopwords]\n",
        "  normalized = []\n",
        "  for word in sw_removed:\n",
        "    changed = False\n",
        "    for n in norm: ## 두번 이상 나온 이모티콘 정규화\n",
        "      if n in word:\n",
        "        normalized.append(n)\n",
        "        changed = True\n",
        "    if 'ㅜ' in word: ## ㅜ를 ㅠ로 변환\n",
        "      normalized.append('ㅠ')\n",
        "      changed = True\n",
        "    \n",
        "    if not changed:\n",
        "      normalized.append(word)\n",
        "  return normalized"
      ],
      "metadata": {
        "id": "zmSADqdBu5bf"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = []\n",
        "norm = ['!', '?', 'ㅠ', 'ㅋ', 'ㅎ', '.', '~', ',', ';', '^', 'ㄷ', 'ㅡ', 'ㅉ']\n",
        "\n",
        "sent_m = []\n",
        "\n",
        "for sent in sentence:\n",
        "  m = sent_to_morphs(sent, stopwords, norm)\n",
        "\n",
        "  sent_m.append(m)\n",
        "\n",
        "sent_m_tok = tokenizer.texts_to_sequences(sent_m)\n",
        "sent_m_pad = pad_sequences(sent_m_tok, maxlen=30)"
      ],
      "metadata": {
        "id": "0YUHqSpEvVZA"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "\n",
        "pred1 = model.predict(sent_m_pad)\n",
        "pred2 = model2.predict(sent_m_pad)\n",
        "\n",
        "\n",
        "pred.append(np.argmax(pred1,axis=1))\n",
        "pred.append(np.argmax(pred2,axis=1))\n",
        "\n",
        "result = []\n",
        "\n",
        "result.append(enc.inverse_transform(pred[0]))\n",
        "result.append(enc.inverse_transform(pred[1]))"
      ],
      "metadata": {
        "id": "49VcUtulvkrg"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentence)):\n",
        "  print(sentence[i])\n",
        "  print('\\t\\t\\t\\t\\t\\t\\t\\t', result[0][i], result[1][i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgeogTFLwzlu",
        "outputId": "1ee72f53-d825-46b3-d02c-962f5feb5df7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정말 행복해\n",
            "\t\t\t\t\t\t\t\t 행복 행복\n",
            "정말 슬퍼\n",
            "\t\t\t\t\t\t\t\t 슬픔 행복\n",
            "정말 무서워\n",
            "\t\t\t\t\t\t\t\t 공포 슬픔\n",
            "정말 싫어\n",
            "\t\t\t\t\t\t\t\t 행복 행복\n",
            "정말 화가 나\n",
            "\t\t\t\t\t\t\t\t 놀람 혐오\n",
            "깜짝 놀랐어\n",
            "\t\t\t\t\t\t\t\t 행복 행복\n",
            "종이는 하얗다\n",
            "\t\t\t\t\t\t\t\t 중립 중립\n",
            "ㅋㅋㅋ 유재석 김태호 조합은 믿고 보는거지~\n",
            "\t\t\t\t\t\t\t\t 행복 행복\n",
            "그저께 어머니가 돌아가시고 세상을 잃은 기분입니다..\n",
            "\t\t\t\t\t\t\t\t 놀람 공포\n",
            "아이가 어제부터 토를 계속 하는데 어떻게 해야하죠??\n",
            "\t\t\t\t\t\t\t\t 공포 공포\n",
            "이놈이나 저놈이나 다 똑같은 놈들이야\n",
            "\t\t\t\t\t\t\t\t 분노 분노\n",
            "범죄자들 얼굴을 왜 가리나? 신상공개하라!!\n",
            "\t\t\t\t\t\t\t\t 분노 분노\n",
            "이런거보면 참 우주는 대단한 듯.. 37억이라는 세월.. 짐작도 안간다.\n",
            "\t\t\t\t\t\t\t\t 놀람 혐오\n",
            "저는 법학도이고 현재는 로스쿨에 재학중입니다.\n",
            "\t\t\t\t\t\t\t\t 슬픔 공포\n",
            "넘넘 추카해요~~~^^\n",
            "\t\t\t\t\t\t\t\t 행복 행복\n",
            "하.... 나 시험 개망햇다...ㅠ..\n",
            "\t\t\t\t\t\t\t\t 슬픔 슬픔\n",
            "나 이번달에 생리를 안하는데?? 어떡하징 ㄷㄷ\n",
            "\t\t\t\t\t\t\t\t 공포 공포\n",
            "표절가수 얼굴 보기도 실타ㅋㅋ 나오지 마라\n",
            "\t\t\t\t\t\t\t\t 행복 중립\n",
            "아 징짜 ㅡㅡ 초딩은 사람도 아님?\n",
            "\t\t\t\t\t\t\t\t 놀람 놀람\n",
            "헐 마라탕 위생 문졔 있다구?? 나 마라탕 어제도 먹엇는데??\n",
            "\t\t\t\t\t\t\t\t 놀람 놀람\n",
            "이번네 삼성에서 새로운 핸드폰이 출시됩비다.\n",
            "\t\t\t\t\t\t\t\t 놀람 놀람\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uHC5rcpww5Lb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}